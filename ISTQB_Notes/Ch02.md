# Chapter Two

## 2.1. Testing in the Context of a Software Development Lifecycle

> SDLC: an abstract high-level representation of the SW development process.
> *Sequential SDLC Models:* Waterfall & V-Model
> *Iterative Development Models:* Spiral Model & protyping.
> *Incremental Development Models:* Unified Process.
> *Iterative-incremental Development Model:* Agile.

### 2.1.1. Impact of the Software Development Life Cycle on Testing
> Choice of SDLC impacts:
1. **Scope and timing** of test activities. [inverse relation between scope and timing]
1. **Level of detail** of test docs. [agile does not care about deep docs unlike waterfall] 
1. **Test techniques & test approach.**
1. **Level of test automation.**  [agile relies heavily on automation unlike waterfall]
1. **Roles of a tester.**

> In *sequential development models*:
> > Initial phases &rarr; testers participate in **reviews, test analysis, test design**
> > *Code is created in later phases*
> > **dynamic testing** cannot be performed early in the SDLC.

> In *iterative-incremental development models [like agile]:*
> > **Both static & dynamic testing** can be performed at all test levels.
> > Things that make regression testing easier like *extensive test automation && lightweight work product docs* are favored.
> > *Most of manual testing is done using* **experience-based testing techniques** that do not require prior test analysis and design.


### 2.1.2. Software Development Lifecycle & Good Testing Practices

> Good testing practices (independent of chosen SDLC model):
1. *For every development activity, there is a corresponding test activity* [to make them subject to quality control]
1. *Different test levels have different test objectives*.
1. *Test analysis & design for a given level begins during the corresponding development phase of SDLC.* [adhering to the principle of early testing]
1. *Testers are involved in reviewing work products as soon as drafts of this docs are available* [to make early testing & defect detection support Shift-Left Strategy (fancy way to say early testing)]

### 2.1.3. Testing as a Driver for Software Development

> Test-Driven Development (TDD): [on scale of unit testing --- test cases are called `examples`]
1. code is written to match test cases instead of extensive software design.
1. tests are written before code.

> Acceptance Test-Driven Development (ATDD):
1. Tests are derived from acceptance criteria.
1. Tests are written before the part of the app is developed to make sure it satisfies the tests.

> Behaviour-Driven Development (BDD):
1. Desired app behaviour is expressed via test cases written in simple words in order for stakeholders to understand them. [in Given/When/Then format]
1. Deez test cases are translated into executable tests.


### 2.1.4. DevOps and Testing

> DevOps Tools: Jera (Management) && Jenkins (Build) && Sellenium (Automation)

> Benefits of DevOps in testing:
1. fast feedback on code quality + if changes impact existing code.
1. CI promotes a shift-left approach in testing by *encouraging devs to submit high quality code accompanied by component tests & static analysis.*
1. Pushing for CI/CD.
1. Automation through a delivery pipeline reduces the need for repetitive manual testing.
1. Risk in regression is minimized due to the scale/range of automated regression tests.

> Risks/Challenges of DevOps:
1. Delivery pipeling must be defined and established.
1. CI/CD tools must be introduced and maintained.
1. Test automation requires additional resources and can be difficult to establish/maintain.

> Even with the high level of automated testing provided by DevOps, **manual testing is still needed**.


### 2.1.5. Shift-Left Approach

> Shift-Left == Early Testing.

> Good practices to achieve shift-left:
1. Reviewing the specs from the perspective of testing to find potential defects [ambiguity/incompleteness/inconsistencies].
1. Writing test cases before code is written.
1. Using CI/CD to get fast feedback + automated component tests upon code submissions.
1. Static testing code *before* dynamic testing.
1. Performing non-functional testing starting at the component test level.

> It is important that *stakeholders* are convinced by shift-left approach since it requires extra training/effort/cost at the beginning.



### 2.1.6. Retrospectives & Process Improvement
> Retrospectives (post-project meetings) discuss:
1. What was successful, and should be retrained/continued?
1. What was NOT successful, and should be improved?
1. How to incorporate the improvements & retrain successes in the future?

> Deez results are in **test completion report**.
> Retrospectives are critical for the success of CI implementation.

> Benefits of Retrospectives in Testing:
1. Increased Test effectiveness/efficiency.
1. Increased SW quality.
1. Team bonding & learning.
1. Improved quality of the test basis.
1. Better cooperation between development and testing.


## 2.2. Test Levels & Test Types

> In *sequential SDLC models* &rarr; test levels are defined such that the **exit criteria** of one level are part of the **entry criteria** for the next level.
> In *iterative SDLC models* &rarr; *this does not apply* as development activities span multiple test lest levels and they may overlap.

### 2.2.1. Test Levels
1. `Component Testing` [aka unit testing] &rarr; focuses on testing components in isolation; requiring stuff like *unit test frameworks & test harnesses.* **Component testing is normally performed by devs in their development environment.**
1. `Component Intergration Testing` [aka unit integration testing --- also performed by devs] &rarr; focuses on testing the interfaces & interactions between components. It is heavily dependent on the *integration strategy approach like:* **bottom-up[from mini stuff to big stuff], top-down[from big to mini to know what it needs], big-bang[separate stuff that are integrated together and explode in your face most of the time!].**
1. `System Testing` &rarr; focuses on the overall behaviour & capabilities of the entire system/product; including *functional testing of end-to-end tasks* and *non-functional testing of quality characteristics [tested on a complete system in environment + can simulate subsystems].* **System testing can be performed by an independent team.** 
1. `System Integration Testing` &rarr; focuses on testing the *interfaces* of the system + other systems/external services. **System integration testing requires test environment similar to operational environment.**
1. `Acceptance Testing` &rarr; focuses on validation [building the right product] + demonstrating readiness for deployment [aka making sure the system fulfills the user's needs]. It should be *performed by the intended users.* Its forms are: **User Acceptance Testing[user has their acceptance criteria], Operational Acceptance Testing[come to test it on my grounds], Contractual & Regulatory Acceptance Testing[it meets rules and regulations], Alpha Testing[their tester on our grounds], Beta Testing[our testers on their grounds].**
> Test levels are distinguished by: *test object, test objectives, test basis, defects & failures, approach & responsibilities.*

### 2.2.2. Test Types
> Functional Testing: is the testing of **WHAT the test object should do.**
> > Main objective is to check the functional completeness/correctiveness/appropriateness.

> Non-Functional Testing: is the testing of **how well the system behaves**
> > Main objective is checking the non-functional SW quality characteristics as provided by *ISO/IEC25010*:
1. Performance Efficiency.
1. Compatibility.
1. Usability.
1. Reliability.
1. Security.
1. Maintainability.
1. Portability.
> > Non-Functional Testing should follow early testing since non-functional defects are a pain in the neck when discovered late.

> Test Types are Functional & Non-Functional.
> Test Techniques are Black-Box & White-Box.

> Black-Box Testing [dynamic testing --- specs-based]: is deriving/performing tests based on specifications of test object as found in docs.
> > Main objective is to check the system's behaviour vs its specifications.

> White-Box Testing [dynamic testing --- performance-based]: is deriving/performing tests based on the internal structure of the system [code, architecture, work/data-flows].
> > Main objective is to cover the internal structure of the systems with tests that ensures it reaches acceptance level.

> All of deez `Test Types` can be applied to **all test levels**.

### 2.2.3. Confirmation Testing & Regression Testing
> Confirmation Testing: confirms that a defect has been successfully fixed.
> > Depending on the risk in said defect, the following can be done:
1. executing all test cases that previously have failed due to the defect.
1. adding new tests to cover any changes that were needed to fix the defect.
> > If **we are short on time/money,** confirmation testing can be restricted to just *repeating the steps/tests that should reproduce the failure caused by the defect to check that it does not occur.*

> Regression Testing: confirms that the changes [fixes] do not impact the already-working parts of the system.
> > Regression testing is not restricted to the test object itself but can be related to the **environment.**
> > It is better to first *perform an impact analysis [shows which parts of SW could be affected] to optimize the extent of regression testing.*
> > Number of regression test cases increase with each iteration/release + can be automated [when applying devOps] + should start early.

## 2.3. Maintenance Testing
> Categories of Maintenance Testing:
1. Corrective.
1. Adaptive to changes in environment.
1. Improve performance/maintainability [hot fixes].

> **impact analysis** may be done before changes are made to decide if they SHOULD be made based on the consequences of said changes on other parts of the system.

> Scope of Maintenance Testing depends on:
1. Degree of risk in change.
1. Size of system.
1. Size of change.

> Triggers for maintenance & maintenance testing can be classified as follows:
1. Modifications &rarr; planned enhancements (release-based), corrective changes, hot fixes.
1. Upgrades/migrations of the operational environment &rarr; from one platform to another which requires tests associated with the new environment + changed SW, tests of data conversion when data is from another application is migrated into the system being maintained.
1. Retirement when an app reaches EOL &rarr; requiring testing of data archiving for long data-retention is required + testing of restore/retrieval procedures.
