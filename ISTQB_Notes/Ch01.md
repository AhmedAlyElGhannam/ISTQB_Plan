# Chapter 01

## 1.1. What is Testing?

> Software testing `assesses software quality` and helps reducing the risk of software failure in operation.

> Software testing is a set of activities to discover defects and `evaluate the quality` of software artifacts aka `test objects.`

> A common misconception about testing is that it only consists of executing tests, **but** it includes activities other than running software and checking test results.

> Another common misconception about testing is that it focuses entirely on verifying the test objects; **testing also focuses on `validation`**

> Verification == checking if the system meets the requirements. `(are we building the product right?)`

> Validation == checking if the system meets the users' and/or stakeholders' needs in its operational environment aka as a finished product. `(are we building the right product?)`

### 1.1.1. Test Objectives
1. Evaluating `work products` like:
	1. requirements,
	1. user stories,
	1. designs,
 	1. code.

1. Triggering `failures` and finding `defects`.
1. Ensuring that a certain `test object` is fully covered as per the requirements.
1. `Reducing the level of risk` in software.
1. `Verifying` that a specific requirement (contractual/legal/regulatory) is fulfilled.
1. `Providing info to stakeholders` to allow them to make informed decisions.
1. `Building confidence.`
1. `Validating` that a test object meets the expectations of stakeholders.

> Test objectives can vary depending on: `product being tested`, `test level`, `risks`, `SDLC` being followed, `business-related factors` (corporate structure, competitive considerations, time to market).

### 1.1.2. Testing \& Debugging
> Testing: triggers `failures` that are caused by `defects` in software `(dynamic testing)`  **or** *can directly find defects in the test object `(static testing)`*
> Debugging: Finding the `causes` of a `failure/defect` and `eliminating` them. 
Debugging process involves:
	1. Reproduction of a failure.
	1. Diagnosis (finding the `root cause`).
	1. Fixing the cause.

> Confirmation tests are performed to check if the failure/defect was fixed.
> Regression tests are performed to check if the failure/defect fix had impacted other parts of the system **causing a failure in them.**
> When `static testing` identifies a defect + debugging to remove it, **there is no need for reproduction or diagnosis since static testing directly finds defects and `cannot cause failures.`**

## 1.2. Why is Testing Necessary?

> Testing is a form of `quality control`.

### 1.2.1. Testing's Contributions to Success

> Testing is a cost-effective means of `finding defects`; which can then be `removed` by *debugging* &rarr; indirectly contributing to higher quality.

> Testing is a means of directly evaluating the quality of a test object throughout SDLC &rarr; contributing to decisions to move to next stage (such as release) of SDLC.

> Testing provides users with an indirect representation of a project in development &rarr; ensuring the users needs are considered throughout the SDLC.

### 1.2.2. Testing \& Quality Assurance (QA)

> Testing is a form of `Quality Control (QC)` **NOT** `Quality Assurance (QA)`.
Quality Control is:
	1. product-oriented,
	1. focuses on achieving appropriate levels of quality.

Quality Assurance is:
	1. process-oriented,
	1. improving and implementing processes,
	1. if followed correctly, it will produce good results.
> QA is the responsibility of everyone on a project.

> Test results are used by QA and QC &rarr; QC `fixes defects` & QA `provides feedback` on how well the development and/or test processes are performing.

> **Test** results are used by both `QA` & `QC`.

### 1.2.3. Errors, Defects, Failures, and Root Causes
 
> Humans make `errors`/`mistakes` &rarr; **causing** `failures`.
Causes of human errors:
	1. time pressure,
	1. complexity of work products,
	1. processes,
	1. infrastructure or interactions,
	1. are tired or lack training.

> `defects` in documentation &rarr; requirements specification or test script.
> `defects` in source code
> `defects` in build artifact &rarr; build file.
> `defects` produced earlier in SDLC lead to defective artifacts later in LC.

> `Failures` are not only caused by errors/defects; they can be caused by `environmental conditions`.

> `Root Cause` is a fundamental reason for the occurance of a problem. **(identified via root cause analysis by the DEVELOPER)**

## 1.3. Testing Principles
1. `Testing shows the presence **not absence** of defects`.
1. `Exhaustive testing is impossible` &rarr; testing everything is not practical; testing efforts should be prioritized.
1. `Early testing saves time and money` &rarr; the cost of quality is reduced when static/dynamic testing is conducted as early as possible.
1. `Defects cluster together` &rarr; Pareto principle applies to defects.
1. `Tests wear out` &rarr; pesticide paradox aka the effectiveness of tests is low when detecting new defects; tests need to be **constantly** modified and updated. *however,* repeating the same test can be benefitial like with `regression testing`.
1. `Testing is context dependent` &rarr; no universal approach for testing.
1. `Absence of defects fallacy` &rarr; the misconception that verification **ensures** the success of a system; the product could *still* not fulfill the users' expectations/needs.

## 1.4. Test Activities, Testware, & Test Roles

### 1.4.1. Test Activities & Tasks

> Testware: documentation
> Test Conditions: smallest thing to be tested


1. `Test Planning` &rarr; Defining *test objectives* + *selecting the best approach to achieve said objectives*. [collection of rules testers have to follow]
1. `Test Monitoring & Control` &rarr; *Monitoring* involves comparing planned test activities with actual progress [produces test progress report] &rarr; *Control* involves the actions necessary to meet testing objectives. [corrective actions]
1. `Test Analysis` &rarr; Analysing *test basis* to prioritize *test conditions*.  [What to test?]
1. `Test Design` &rarr; converting *test conditions* into *test cases*. [How to test?]
1. `Test Implementation` &rarr; Acquiring *testware* necessary for test execution + preparing test environment.
1. `Test Execution` &rarr; *running* the tests. [producing a bug report]
1. `Test Completion` &rarr; shutting down test environment, analysing previous test activities, and creating a report. [test completion report]

### 1.4.2. Test Process in Context

> Testing is carried out depending on the following factors. &rarr; **impacting test strategy, test techniques, degree of test automation, required level of coverage, level of detail in docs/reports.**
1. *Stakeholders.*
1. *Team members*.
1. *Business domain*.
1. *Technical factors.*
1. *Project constraints.*
1. *Organizational factors.*
1. *SDLC.*
1. *Tools*.

### 1.4.3. Testware

> Test Basis [input of activity]: 
> Testware [output of activity]: output work products from test activities; including:


1. `Test Planning` work products &rarr; test plan, test schedule, risk register [risk likelihood + risk impact + info about risk mitigation], entry & exit criteria.
1. `Test Monitoring & Control` work products &rarr; test progress reports, docs of control directives, risk info.
1. `Test Analysis` work products &rarr; prioritized test conditions [acceptance criteria], defect reports regarding defects in `test basis`.
1. `Test Design` work products &rarr; prioritized test cases, test charters, coverage items, test data requirements, test environment requirements. [test requirements + sorted list of test cases]
1. `Test Implementation` work products &rarr; test procedures, automated test scripts, test suites, test data, test execution schedule, test environment elements **[stubs, drivers, simulators, service virtualization]**.
1. `Test Execution` work products &rarr; test logs, defect reports.
1. `Test Completion` work products &rarr; test completion report.


> Planning: [input == requirements] && [output == plan + schedule + risk register]
> Monitoring: [input == plan] && [output == test progress report]
> Control: [input == test progress report] && [output == corrective actions]
> Analysis: [input == plan + requirements] && [output == test conditions]
> Design: [input == test conditions] && [output == test cases]
> Implementation: [input == test cases] && [output == test procedure + automation + stubs & drivers]
> Execution: [input == test procedures + automation + stubs & drivers] && [output == test logs && defect reports] 
> Completion: [input == logs + reports] && [output == test completion report]


### 1.4.4. Tracability between Test Basis & Testware
> Establishing && maintaining traceability between test basis and testware is important for monitoring and control.
> Traceability of `test cases` to `requirements` to verify that requirements are covered by test cases.
> Traceability of `test results` to `risks` to measure the level of risk in a test object.
> Traceability makes progress & completion reports more understandable.

> Requirements Tracability Matrix: shows if each requirement is covered by at least one test case.


### 1.4.5. Roles in Testing
> Test Management Role
1. `Takes responsibility` of the *test process, test team*.
1. This role is mainly focused on `test planning`, `test monitoring and control`, `test completion`.
1. In Agile SD, some of deez management roles are handled by the `Agile team` + tasks that span multiple teams/the entire organization can be performed by `test managers outside of the development team.`

> Testing Role
1. Takes responsibility for technical aspects of testing.
1. This role is mainly focused on `test analysis`, `test implementation`, `test execution`.

> *Test Management* role can be performed by a `team leader`, `test manager`, `development manager`.

## 1.5. Essential Skills and Good Practices in Testing

### 1.5.1. Generic Skills Required for Testing
1. `Testing knowledge` &rarr; like using *test techniques*.
1. `Curiosity + attention to details` &rarr; to identify defects.
1. `Good communication skills + active listening + being a team player` &rarr; interact effectively with stakeholders, convey information to others.
1. `Analytical thinking + critical thinking + creativity`.
1. `Technical knowledge` &rarr; increase efficiency of testing by using appropriate test tools.
1. `Domain knowledge` &rarr; communicate with business representatives.

> Testers are often the bearers of bad news &rarr; some perceive testing as a `destructive activity` even if it contributes greatly to project success.
> This view is improved by *communicating defects and failures in a constructive way.*

### 1.5.2. Whole Team Approach
> An important skill for a tester is **the ability to work in a team and contribute to their goals.** 
> Whole-team approach &rarr; a practice steming from extreme programming, **any team member with the necessary knowledge and skills can perform any task and everyone is responsible for the quality.**
> Whole-team approach `improves dynamics`, `enhances communication/collaboration within the team`, `creates synergy` *by allowing the skillsets within the team to be leveraged for the benefit of the project.*
> `Testers work closely with other team members to ensure the desired quality levels are achieved` &rarr; *collaborating with business representatives to help in creating acceptance tests* + *working with developers to decide on `test strategy` & `test automation approaches`.*
> Whole-team approach **may not be appropriate depending on the context** &rarr; a high level of test independence is needed in situations such as *safety-critical* applications.

### 1.5.3. Independence of Testing
> `A certain degree of independence makes testers more effective at finding defects` &rarr; due to the developer & tester's cognitive biases. **Independence is no a replacement for familiarity since developers can effectively find defects in their code.**

> `Work products can be tested by:` 
1. their author (no independence),
1. author's peers from the same team (some independence),
1. testers outside the author's team but within the same organization (high),
1. testers outside the organization (very high)

> Level of test independence is determined based on project context + budget + resources.

> Best practice: `developers performing component & component integration testing`, `test team performing system & system integration testing`, and `business representatives performing acceptance testing.`

> Benefit of independent testing:
1. high likelyhood of finding defects/failures
1. verify, challenge, and disprove assumptions made by stakeholders during specification and implementation of system.

> Drawbacks of independent testing:
1. independent testers may be isolated from the development team; leading to `lack of collaboration, communication problems, or adversarial relationships.`
1. deez testers can be seen as a bottleneck or blamed for delays.


