# Chapter Four

## 4.4. Experience-based Test Techniques

### 4.4.1. Error Guessing
> Error Guessing: a technique used to anticipate the occurance of errors/defects/failures based on **the tester's knowledge**; including: \
1. How the app has worked in the past.
1. The types of errors the developers tend to make and the types of defects that result from these errors.
1. The types of failures that have occured in similar apps.

> Errors, defects, failures may be related to: \
1. inputs &rarr; correct input not accepted || wrong/missing parameters.
1. output &rarr; wrong format/result.
1. logic &rarr; missing cases || wrong operator.
1. computation &rarr; incorrect operand || wrong computation.
1. interfaces &rarr; parameter mismatch || incompatible types.
1. data &rarr; incorrect initialization || wrong type.


### 4.4.2. Exploratory Testing 
> Tests are designed, executed, and evaluated **simultaneously while the tester learns about the test object.** \
> Sometimes, it is conducted using **session-based approach**:
1. test charters [containing test objectives] are used for guidance.
1. followed by debriefing involving testers and stakeholders.

> Exploratory testing is useful **when there are few specs || significant time pressure on testing.** \
> Exploratory testing can incorporate other test techniques.


### 4.4.3. Checklist-based Testing 
> In checklist-based testing, a tester **designs, implements, and executes tests to cover test conditios from a checklist.**

> Checklists **should not** contain items that can be checked automatically, items better suited as entry/exit criteria, or items that are too general.

> Checklists are phrased in the form of a **question**.

> Checklist items may refer to requirements, graphical interface properties, quality characteristics or other forms of test conditions.

> Some checklist entries may gradually become less effective over time **because** the developers will learn to avoid making the same errors.

> New entries are added to reflect newly found high severity defects. &rarr; **checklists are regularly updated based on defect analysis BUT IT SHOULD NOT BE TOO LONG.**

> In absence of detailed test cases, checklist-based testing can provide guidelines + some degree of consistency to testing.


## 4.5. Collaboration-based Test Approaches

### 4.5.1. Collaboration User Story Writing

> Critical aspects of user stories: **[3 C's]**
1. Card &rarr; medium describing a user story [index card || entry in an electronic board]
1. Conversation &rarr; explains how the software will be used [documented || verbal]
1. Confirmation &rarr; acceptance criteria.

> Authorship techniques: brainstorming + mindmapping.

> Qualities of a good user story: **[INVEST]**
1. Independent,
1. Negotiable,
1. Valuable,
1. Estimatable,
1. Small,
1. Testable.

### 4.5.2. Acceptance Criteria
> aka the conditions that an implementation of the user story must meet to be accepted by stakeholders.

> May be viewed as *the test condition that should be exercised by the tests.*

> It is a result of the `Conversation` [aka the 3rd critical aspect of user stories] and written in `Confirmation`.

> Acceptance criteria are used to:
1. Define the scope of the user story.
1. Reach consensus among stakeholders.
1. Describe both +ve && -ve scenarios.
1. Serve as a basis for the user story acceptance testing.
1. Allow accurate planning and estimation.

> Ways to write acceptance criteria:
1. Scenario-oriented [given/when/then]
1. Rule-oriented [bullet poin verification list || tabulated form of input-output mapping]

> A team **can use a different way to write acceptance criteria as long as they are well-defined and unambiguous.**

### 4.5.3. Acceptance Test-Driven Development

> ATDD is a **test-first approach.**

> Test cases are created by teams **with different perspectives** + **they can be executed manually || automated.**

> Specification Workshop [first step] &rarr; user story & its acceptance criteria are analyzed, discussed, and written by team members. **incompleteness && ambiguity && defects are resolved here.**

> Test Case Creation &rarr; done by the whole team || a single tester **based on acceptance criteria.**

> First test cases are *+ve* then *-ve* then non-functional.

> **No two test cases should describe the same characteristics of the user story.**


