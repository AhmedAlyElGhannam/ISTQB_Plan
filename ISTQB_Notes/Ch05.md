# 5. Managing the Test Activities

> policy strategy plan (from high to low)
> plan is per project
> policy & strategy are higher levels

## 5.1. Test Planning

### 5.1.1. Purpose and Content of a Test Plan

> A test plan accomplishes the the following: \
1. Document the means && schedule for achieving test objectives.
1. Ensure that the performed test meets the established criteria.
1. Demonstrates that testing will adhere to the existing test policy && strategy.

> Content of a test plan: \
1. Context of testing &rarr; scope, test objectives, test basis.
1. Assumptions & constraints of the test project.
1. Stakeholders &rarr; roles, responsibilities, relevance to testing, hiring & training needs.
1. Communication &rarr; forms and frequency of communication, documentation templates.
1. Risk register &rarr; product & project risks.
1. Test approach &rarr; test levels, types, techniques, deliverables, entry/exit criteria, independence of testing, test data/environment requirements, deviations from organizational test policy, test strategy.
1. Budget & schedule.

### 5.1.2. Tester's Contribution to Iteration & Release Planning

> Release Planning: looks ahead of the release of a product, defines & redefines the **product backlog [list of all the project's user stories]**, and may involve refining larger user stories into a set of smaller user stories + **it serves as the basis for the test approach & test plan across all iterations**

> Roles of testers participating in Release Planning: \
1. writing testable user stories & acceptance criteria,
1. participate in project and quality risk analyses,
1. estimate test effort associated with user stories, 
1. determine the test approach,
1. plan the testing for the release.

> Iteration Planning: looks ahead to the end of a single iteration, is concerned with **iteration backlog [list of user stories for this iteration].**

> Roles of testers participating in Iteration Planning: \
1. detailed risk analysis of user stories,
1. determine the testability of user stories,
1. break down user stories into testing tasks,
1. estimate test effort fr all testing tasks,
1. identify & refine functional and non-functional aspects of the test object.

### 5.1.3. Entry Criteria & Exit Criteria

> Entry Criteria: define the preconditions for undertaking a given activity.
> If entry criteria are not met, **the activity will be more difficult, time-consuming, costly, and riskier.**


> Exit Criteria: define what must be achieved in order to declare an activity completed.

> **Entry & Exit criteria should be defined for each test level & will differ based on test objectives.**

> Typical Entry Criteria: \
1. availability of resources &rarr; people, tools, environments, test data, budget, time.
1. availability of testware &rarr; test basis, testable requirements, user stories, test cases.
1. initial quality level of a test object &rarr; all **smoke tests** have passed.

> Typical Exit Criteria: \
1. measures of thoroughness &rarr; achieved level of coverage, number of unresolved defects, defect density, number of failed test cases.
1. completion criteria &rarr; planned tests have been executed, static testing has been performed, all defects found are reported, all regression tests are automated.

> **Running out of time or budget can also be viewed as valid exit criteria even without other exit criteria being satisfied.**

> In Agile SD, exit criteria is called **Definition of Done**.

> In Agile SD, entry criteria is called **Definition of Ready.**

### 5.1.4. Estimation Techniques

> Test Effort Estimation: involves predicting the amount of test-related work needed to meet the objectives of a test project. **must be clear to stakeholders with estimation prone to margin of error**

> Estimation Based on Ratios: \
1. metrics-based technique,
1. figures are collected from previous projects within the organization; making it possible to derive standard ratios for similar projects.
1. best source in the estimation process.

> Extrapolation: \
1. metric-based technique.
1. measurements are made as early as possible in the current project to gather data.
1. with enough observations, the required effort for the remaining work can be approximated by extrapolating the data.
1. this method is suitable for **iterative SDLCs.**

> Wideband Delphi: \
1. iterative & expert-based technique.
1. experts make **experience-based estimations**.
1. each expert estimates the effort in isolation THEN the results are collected, discussed, then if some estimations deviated the process is **repeated based on feedback from the discussion until common ground is reached.**
1. **Planning Poker** is a variant of Wideband Delphi used in Agile SD [estimates are made using cards with numbers that represent the effort size].

> Three-point Estimation: \
1. expert-based technique.
1. 3 estimations are made by experts &rarr; most optimistic estimation [a], most likely estimation [m], and the most pessimistic estimation [b].
1. The final estimation [e] is the weighed arithmetic mean of the other estimations.
	```
	E = (a + 4m + b) / 6
	```
1. The advantage of this technique is that it allows experts to calculate the measurement error **SD**:
	```
	SD = (b - a) / 6
	```

### 5.1.5. Test Case Prioritization
> Test Case Prioritization Strategy: \
1. Risk-based prioritization &rarr; order of test execution is based on the results of risk analysis. Test cases covering the most important risks are executed first.
1. Coverage-based Prioritization &rarr; order of test execution is based on coverage [aka statement/branch coverage]. Tests achieving the highest coverage are executed first. **Another variant called `Additional Coverage Prioritization`, test case that achieves the highest coverage is executed first THEN THE NEXT TEST CASE TO BE EXECUTED IS THE ONE THE ACHIEVES THE HIGHEST ADDITIONAL COVERAGE.**
1. Requirements-based Prioritization &rarr; order of execution is based on the priorities of the requirements that are covered by a test case. **Requirement priorities are defined by by stakeholders.**

> Ideally, test cases would be ordered to run based on their priority level. **However,** this will not happen if the test cases have dependencies on one another.

> Order of test execution must take into account the **availability of resources.**

### 5.1.6. Test Pyramid

> Definition: a model showing that different tests may have different granuality.

> This model supports the team in **test automation** && **test effort allocation** by showing that different goals are supported by different levels of test automation.

> Pyramid layers represent groups of tests &rarr; the higher the layer, the lower the test granuality && test isolation && test execution time.

> Tests in the bottom layer are small, isolated, fast, and check a small piece of functionality &rarr; **a lot of them are needed to achieve reasonable coverage.**

> Top layer represents complex, high-level, end-to-end tests [generally slower than lower layer tests + check a chonky amout of functionality &rarr; only a few are needed to achieve reasonable coverage.]

> OG Pyramid Model: three layers &rarr; **unit tests, service tests, UI tests.**

> Another model defines unit/component tests, integration/component integration, and end-to-end tests.

### 5.1.7. Testing Quadrants

> Q1 [unit/component/component integration testing] &rarr; **[technology facing] [supports the team] [should be automated]**
> Q2 [Functional Tests && user story tests && API testing && simulations] &rarr; **[business facing] [supports the team] [can be automated || manual]**
> Q3 [exploratory testing && usability testing && user acceptance testing] &rarr; **[business facing] [critique the product] [should be manual]**
> Q4 [smoke tests && non-functional tests] &rarr; **[technology facing] [critique the product] [automated]**.

## 5.2. Risk Management

### 5.2.1. Risk Definition & Risk Attributes

> Risk: a potential event, hazard, threat, or situation whose occurance causes an adverse effect.

> Risks are characterized by two factors: \
1. Risk Likelihood &rarr; the probability of the risk occurance **]0,1[**.
1. Risk Impact (harm) &rarr; the consequences of this occurance.

### 5.2.2. Project Risks and Product Risks [aka types of risks]

> Project Risks [team]: risks related to management and control of the project [may impact project schedule || budget || scope; impacting the ability to achieve its objectives]; including: \
1. Organizational issues &rarr; delays in work products deliveries, inaccurate estimates, cost-cutting.
1. People issues &rarr; insufficient skills, conflicts, communication problems, shortage of staff.
1. Technical issues &rarr; scope creep, poor tool support.
1. Supplier issues &rarr; third-party delivery failure, bankruptcy of the supporting company.

> Product Risks [code]: risks related to product quality characteristics; including: \
1. missing or wrong functionality.
1. incorrect calculations.
1. runtime errors.
1. poor architecture.
1. inefficient algorithms.
1. inadequate response time.
1. poor user experience.
1. security vulnerability.

> Negative consequences of the occurance of product risks: \
1. User dissatisfaction.
1. Loss of revenue, trust, reputation.
1. Damage of third parties.
1. High maintenance costs, overload of the helpdesk.
1. Criminal penalties.
1. Physical damage, injuries, or even death.

### 5.2.3. Product Risk Analysis

> Goal of product risk analysis &rarr; **provide awareness of product risk to focus the testing effort in a way that minimizes the risk levels**

> Product risk analysis begins early in SDLC.

> Product risk analysis consists of `Risk Identification` and `Risk Assessment`.

> Risk Identification: generating a comprehensive list of risks. **stakeholders identify risks by using techniques/tools like brainstorming, workshops, interviews, interviews, cause-effect diagrams**.

> Risk Assessment: categorization of identified risks [helps in assigning mitigation actions **because risks falling into the same category can be mitigated using a similar approach**], determining their risk likelihood, risk impact and level, prioritizing, proposing ways to handle them.

> > Risk Assessment can use a qualitative and/or quantitative approach. *in quantitative, risk level = riskLikelyhood x riskImpact*; *in qualitative approach, riskLevel is determined via riskMatrix*

> Results of Product Risk Analysis: \
1. Determine the scope of testing to be carried out.
1. Determine the particular test levels and propose test types to be performed.
1. Determine the test techniques to be employed and the coverage to achieved.
1. Estimate the test effort required for each task.
1. Prioritize testing in an attempt to find the critical defects as early as possible.
1. Determine whether any activities in addition to testing could be employed to reduce risk.


### 5.2.4. Product Risk Control

> Product Risk Control: measures taken in response to identified/assessed product risks.

> Product Risk Control consists of `Risk Mitigation` and `Risk Monitoring`.

> Risk Mitigation &rarr; implementing actions proposed in risk assessment to reduce risk level.

> Risk Monitoring &rarr; ensures mitigation actions are effective to obtain further info to improve risk assessment + identify emerging risks.

> Response options to risk &rarr; risk mitigation by testing, risk acceptance, risk transfer, contingenct plan.

> Risk Mitigation By Testing actions are: \
1. select testers with the right level of experience and skills suitable for a given risk type.
1. Apply an appropriate level of independence of testing.
1. Conduct reviews and perform static analysis.
1. Apply the appropriate test techniques and test coverage.
1. Apply the appropriate test types addressing the affected quality characteristics.
1. Perform dynamic testing, including regression testing.


## 5.3. Test Monitoring, Test Control & Test Completion

### 5.3.1. Metrics Used in Testing

> Usage of Test Metrics: \
1. show progress against planned schedule and budget,
1. current quality of test object,
1. effectiveness of test activities w.r.t. objectives/iteration goal.

> Test monitoring gathers metrics to support test control & completion.

> Test metrics include: \
1. Project progress metric &rarr; test completion, resource usage, test effort.
1. Test progress metrics &rarr; test case implementation progress, test environment preparation progress, number of test cases run/not run, passed/failed, test execution time.
1. Product quality metrics &rarr; availability, response time, mean time to failure.
1. Defect metrics &rarr; number & priorities of defects found/fixed, defect density, defect detection percentage.
1. Risk metrics &rarr; residual risk level.
1. Coverage metrics &rarr; requirements coverage, code coverage.
1. Cost metrics &rarr; cost of testing, organizational cost of quality.

### 5.3.2. Purpose, Content, & Audience for Test Reports

> Test Progress Report [Test Status Report]: provides info to make modifications to test schedule, resources, test plan when needed if deviations from plan happen. They include: \
1. Test period.
1. Test progress [ahead/behind schedule].
1. Impediments for testing and their workarounds.
1. Test metrics.
1. New/changed risks within testing period.
1. Testing planned for the next period.

> Test Completion Report: summarizes a specific stage of testing (test level, test cycle, iteration) + give info for subsequent testing. They include: \
1. Test summary.
1. Testing and product quality evaluation based on the original test plan [test objectives && exit criteria]
1. Deviations from the test plan.
1. Testing impediments and workarounds.
1. Test metrics based on test progress reports.
1. Unmitigated risks & unfixed defects.
1. Lessons learned that are relevant to the testing.

> **Different audiences require different info in the reports + degree of formality + frequency of reporting** &rarr; [same team --- more frequent && informal] [to superiors --- follow a set template and occurs upon completing the project]

### 5.3.3. Communicating the Status of Testing 

> Choice of means of communicating test status depends on:
1. test management concerns,
1. organizational test strategies, regulatory standards,
 the team itself [in case of self-organizing teams].

> Communication options:
1. Verbal communication.
1. Dashboards &rarr; CI/CD dashboards, task boards, burn-down charts.
1. Electronic communication channels &rarr; email, chat.
1. Online documentation.
1. Formal test reports.

## 5.4. Configuration Management

> Configuration Management [CM]: a discipline for identifying/controlling/tracking work products such as **test plans, test strategies, test conditions, test cases, test scripts, test results, test logs, test reports** as configuration items.

> For a complex configuration items like test environment, CM **records the items it consists of + their relationships/versions.** 

> `If a configuration item is approved for testing, it becomes a baseline and can only be changed through a formal change control process.`

> CM keeps a record of changed configuration items when a new baseline is created **to make it possible to revert to them to reproduce previous test results**

> CM ensures the following in order to support testing: \
1. All configuration items are uniquely identified, version controlled, tracked for changes, related to other configuration items so that traceability can be maintained throughout the test process.
1. All identified documentation and software items are referenced unambiguously in test document.

> Automated CM is implemented as a part of DevOps pipeline.

## 5.5. Defect Management

> Objectives of Defect Reports: \
1. Provide those responsible for handling/resolving reported defects with sufficient information to resolve the issue.
1. Provide a means of tracking the quality of work product.
1. Provide ideas for improvement of the development/test process.

> Contents of a defect report logged during **dynamic testing**: \
1. Unique identifier.
1. Title with a short summary of the anomaly being reported.
1. Date when the anomaly was observed, issuing organization, and author, including their role.
1. Identification of the test object/environment.
1. Context of the defect [test case being run, test activity being performed, SDLC phase, test technique, checklist/test data being used].
1. Description of the failure to enable reproduction and resolution [including the steps taken to detect the anomaly + relevant test logs + database dumps + screenshots/recordings].
1. Expected results vs actual result.
1. Severity of the defect **(degree of impact)** on the interest of stakeholders || requirements.
1. Priority to fix.
1. Status of the defect [open, deferred, duplicate, waiting to be fixed, awaiting confirmation testing, re-opened, closed, rejected].
1. References [test case].

> Data like [identifier, date, author, initial status] are automatically included using defect management tools.
